def main2():
# coding: utf-8

# # Text Summarization in Python
# 
# ## Motivation: 
# The length of textual data is increasing and people have less time. Often the newspaper articles run into a long text of, say 1000 -1200 words. As wearable devices leap to prominence (Google Glass, Apple Watch, to name a few), content must adapt to the limited screen space available on these devices.
# The task of generating intelligent and accurate summaries for long pieces of text has become a popular research as well as industry problem.

# ## Approach: 
# Extractive text summarization is all about finding the more important sentences from a document as a summary of that document.
# Our approach is using the PageRank algorithm to find these 'important' sentences.

# ## Implementation

# ### 1. Importing important libraries

# In[73]:

# numpy library helps in working with arrays: array creation and manipulation
# this implementation uses array for storing the matrices generated as 2-D arrays
# PyPDF2 is a library used for reading the PDF files
# sys library has been used for printing the size of data structures used in the program
    import numpy as np
    import PyPDF2
    import sys


# In[74]:

# matplotlib is a library that is used to visualize the data by drawing graphs of matrix inputs
# we will use it for drawing the matrices generated later in the program 
# %matplotlib inline is a command used to show the graphs in the jupyter notebook

    import matplotlib.pyplot as plt
#get_ipython().magic('matplotlib inline')


# In[75]:

# networkx library helps in working with graphs ...
# and later performing the PageRank algorithm ...
# which is the crux of this implementation to find ...
# the importance of each sentence using their 'rank' as a metric ...
# rank, the output of the method pagerank, is a measure of importance of sentences
# this library has been used in the cell no. ()

    import networkx as nx


# In[76]:

# the PunktSentenceTokenizer library is being imported from the file punkt.py contained in package nltk.tokenize 
# this is used to tokenize the document into sentences

# Tokenization: Tokenization is the process of demarcating and possibly classifying.. 
# sections of a string of input characters. 
# The resulting tokens are then passed on to some other form of processing. 

    from nltk.tokenize.punkt import PunktSentenceTokenizer


# In[77]:

# TfidfTransformer and CountVectorizer libraries are being imported

# CountVectorizer: In this implementation, a CountVectorizer object is being created that ..
# will be used for creating the document-term matrix

# tFidTransformer: In this implementation,TfidfTransformer is used for executing the method fit_transform()... 
# which provides the output as a document-term matrix normalized (value 0-1) according to the TF-IDF
# TF(Term Frequency): the no. of times a term(a word here) appears in the current document(single sentence here)
# IDF(Inverse Document Frequency): the no. of times a term(a word here) appears in the entire corpus
# Corpus: set of all sentences


    from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer


# ### 2.  Function to read the document from user
# Supported formats: .txt, .pdf 
# 
# Input: Takes the name of the file as input. 
# 
# Output: Returns a string output containing the contents of the file.

# In[78]:

# we are going to show an example of how the method is working
# first let's take the document as an input
    # def readDoc():
    #     name = input('Please input a file name: ') 
    #     print('You have asked for the document {}'.format(name))

    # # now read the type of document
    #     if name.lower().endswith('.txt'):
    #         choice = 1
    #     elif name.lower().endswith('.pdf'):
    #         choice = 2
    #     else:
    #         choice = 3
    #     # print(name)
    #     print(choice)
    # # Case 1: if it is a .txt file
        
    #     if choice == 1:
    #         f = open(name, 'r')
    #         document = f.read()
    #         f.close()
            
    # # Case 2: if it is a .pdf file
    #     elif choice == 2:
    #         pdfFileObj = open(name, 'rb')
    #         pdfReader = PyPDF2.PdfFileReader(pdfFileObj)
    #         pageObj = pdfReader.getPage(0)
    #         document = pageObj.extractText()
    #         pdfFileObj.close()
    
    # # Case 3: none of the format
    #     else:
    #         print('Failed to load a valid file')
    #         print('Returning an empty string')
    #         document = ''
    
    #     print(type(document))
    #     return document


# ### 3. Function to tokenize the document
# Input: String of text document
# 
# Output: A list containing sentences as its elements

# In[79]:

# the function used for tokenizing the sentences
# tokenization of a sentence: '''provided in cell() above'''

    def tokenize(document):
    # We are tokenizing using the PunktSentenceTokenizer
    # we call an instance of this class as sentence_tokenizer
        doc_tokenizer = PunktSentenceTokenizer()
    
    # tokenize() method: takes our document as input and returns a list of all the sentences in the document
    
    # sentences is a list containing each sentence of the document as an element
        sentences_list = doc_tokenizer.tokenize(document)
        return sentences_list


# ### 4. Read the document

# In[80]:

# reading a file and 
# printing the size of the file
    f=open('summarizer\\1.txt','r')
    document = f.read()
    l=len(document.strip().split())
    f.close()
    print('The length of the file is:', end=' ')
    print(len(document.strip().split()))


# ### 5. Generate a list of sentences in the document

# In[81]:

# we want to tokenize the document for further processing
# tokenizing the sentence means that we are creating a list of all the sentences of the document.
# Need of tokenizing the document: Initially the document is in just a string format.
# if we want to process the document, we need to store it in a data structure.
# Tokenization of document into words is also possible, but we will go with the tokenizing with the sentences
# Since we want to choose the most relevant sentences, we need to generate tokens of sentences only
    sentences_list = tokenize(document)

# let us print the size of memory used by the list sentences
#     print('The size of the list in Bytes is: {}'.format(sys.getsizeof(sentences_list)))

# # the size of one of the element of the list
#     print('The size of the item 0 in Bytes is: {}'.format(sys.getsizeof(sentences_list[0])))


# In[82]:

# let us see the data type of sentences_list
# It will be list
    print(type(sentences_list))


# In[83]:

# let us analyse the elements of the sentences
# len() method applies on the list and provides the number of elements in the list
    print('The size of the list "sentences" is: {}'.format(len(sentences_list)))


# In[84]:

# print the elements of the list
# If the input document is long, which on realistically will be wrong, we would not like to print the entire document
    for i in sentences_list:
        print(i)


# ### 6. Generate term-document matrix (TD matrix) of the data 

# In[85]:

# Convert a collection of text documents to a matrix of token counts
# fit_transform method of CountVectorizer() class 
# Learn the vocabulary dictionary and return term-document matrix. 
# I/p: An iterable which yields either str, unicode or file objects.
# O/p: The term-document matrix named cv_matrix
    cv = CountVectorizer()
    cv_matrix = cv.fit_transform(sentences_list)


# **So what does CountVectorizer.fit_transform() do?**

# In[86]:

# a demo of what CountVectorizer().fit_transform(text) does
    cv_demo = CountVectorizer() # a demo object of class CountVectorizer

# I have repeated the words to make a non-ambiguous array of the document text matrix 

    text_demo = ["Ashish is good, you are bad", "I am not bad"] 
    res_demo = cv_demo.fit_transform(text_demo)
    print('Result demo array is {}'.format(res_demo.toarray()))

# Result is 2-d matrix containing document text matrix
# Notice that in the second row, there is 2.
# also, bad is repeated twice in that sentence.
# so we can infer that 2 is corresponding to the word 'bad'
    print('Feature list: {}'.format(cv_demo.get_feature_names()))


# In[87]:

# printing the cv_matrix type
# and how it is being stored in memory?
# it is stored in the compressed row format
# compressed row format: 
    print('The data type of bow matrix {}'.format(type(cv_matrix)))
    print('Shape of the matrix {}'.format(cv_matrix.get_shape))
    print('Size of the matrix is: {}'.format(sys.getsizeof(cv_matrix)))
    print(cv.get_feature_names())
    print(cv_matrix.toarray())


# In[88]:

# Tnormalized: document-term matrix normalized (value 0-1) according to the TF-IDF
# TF(Term Frequency): the no. of times a term(a word here) appears in the current document(single sentence here)
# IDF(Inverse Document Frequency): the no. of times a term(a word here) appears in the entire corpus
# Corpus: set of all sentences

    normal_matrix = TfidfTransformer().fit_transform(cv_matrix)
    print(normal_matrix.toarray())


# In[89]:

    print(normal_matrix.T.toarray)
    res_graph = normal_matrix * normal_matrix.T
# plt.spy(res_graph)


# ### 7. Generate a graph for the document to apply PageRank algorithm  

# In[90]:

# drawing a graph to proceed for the textrank algorithm
# nx_graph is a graph developed using the networkx library
# each node represents a sentence
# an edge represents that they have words in common
# the edge weight is the number of words that are common in both of the sentences(nodes)
# nx.draw() method is used to draw the graph created

    nx_graph = nx.from_scipy_sparse_matrix(res_graph)
    nx.draw_circular(nx_graph)
    print('Number of edges {}'.format(nx_graph.number_of_edges()))
    print('Number of vertices {}'.format(nx_graph.number_of_nodes()))
# plt.show()
    print('The memory used by the graph in Bytes is: {}'.format(sys.getsizeof(nx_graph)))


# ### 8. Getting the rank of every sentence using pagerank

# In[91]:

# ranks is a dictionary with key=node(sentences) and value=textrank (the rank of each of the sentences)
    ranks = nx.pagerank(nx_graph)

# analyse the data type of ranks
    print(type(ranks))
    print('The size used by the dictionary in Bytes is: {}'.format(sys.getsizeof(ranks)))

# print the dictionary
    for i in ranks:
        print(i, ranks[i])


# ### 9. Finding important sentences and generating summary

# In[92]:

# enumerate method: returns an enumerate object
# Use of list Comprehensions
# O/p: sentence_array is the sorted(descending order w.r.t. score value) 2-d array of ranks[sentence] and sentence 
# For example, if there are two sentences: S1 (with a score of S1 = s1) and S2 with score s2, with s2>s1
# then sentence_array is [[s2, S2], [s1, S1]]
    sentence_array = sorted(((ranks[i], s) for i, s in enumerate(sentences_list)), reverse=True)
    sentence_array = np.asarray(sentence_array)


# In[93]:

# as sentence_array is in descending order wrt score value
# fmax is the largest score value(the score of first element)
# fmin is the smallest score value(the score of last element)

    rank_max = float(sentence_array[0][0])
    rank_min = float(sentence_array[len(sentence_array) - 1][0])


# In[94]:

# print the largest and smallest value of scores of the sentence
    print(rank_max)
    print(rank_min)


# In[95]:

# Normalization of the scores
# so that it comes out in the range 0-1
# fmax becomes 1
# fmin becomes 0
# store the normalized values in the list temp_array

    temp_array = []

# if all sentences have equal ranks, means they are all the same
# taking any sentence will give the summary, say the first sentence
    flag = 0
    if rank_max - rank_min == 0:
        temp_array.append(0)
        flag = 1

# If the sentence has different ranks
    if flag != 1:
        for i in range(0, len(sentence_array)):
            temp_array.append((float(sentence_array[i][0]) - rank_min) / (rank_max - rank_min))

    print(len(temp_array))


# In[96]:

# Calculation of threshold:
# We take the mean value of normalized scores
# any sentence with the normalized score 0.2 more than the mean value is considered to be
    threshold = (sum(temp_array) / len(temp_array)) + 0.3


# In[97]:

# Separate out the sentences that satiasfy the criteria of having a score above the threshold
    sentence_list = []
    if len(temp_array) > 1:
        for i in range(0, len(temp_array)):
            if temp_array[i] > threshold:
                sentence_list.append(sentence_array[i][1])
    else:
        sentence_list.append(sentence_array[0][1])


# In[98]:

    model = sentence_list


# ### 10. Writing the summary to a new file

# In[101]:

# print(sentence_list)
    summary = " ".join(str(x) for x in sentence_list)
    print(summary)
    
# save the data in another file, names sum.txt
    f = open('summarizer\\sum_.txt', 'a')
#print(type(f))
    f.seek(0)
    f.truncate()
    f.write('-------------------\n')
    f.write(summary)
    f.write('\n')

    f.close


# In[102]:

    # for lines in sentence_list:
    #     print(lines)
    return (summary,l)

# # End of the notebook
if __name__=="__main__":
    main2()